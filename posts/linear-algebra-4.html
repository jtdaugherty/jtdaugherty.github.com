<html>
  <head>
    <title>Linear Algebra, Part 4: Elimination - Derivative Works</title>
    <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/3.2.0/build/cssreset/reset-min.css">
    <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/3.2.0/build/cssbase/base-min.css">
    <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/3.2.0/build/cssfonts/fonts-min.css">
    <link rel="stylesheet" type="text/css" href="/stylesheets/stylesheet.css"/>
    <link rel="alternate" type="application/rss+xml" href="http://jtdaugherty.github.io/feed.xml"/>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config(
  {"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
   tex2jax: {
     element: null,
     preview: "none",
     skipTags: ["script","noscript","style","textarea","pre","code"],
     inlineMath: [
       ['$', '$'],
       ["\\(", "\\)"],
     ],
     displayMath: [
       ['$$', '$$'],
       ["\\[", "\\]"],
     ],
     processEscapes: true,
     ignoreClass: "tex2jax_ignore|dno"
   },
   TeX: {
     extensions: ["AMSmath.js","AMSsymbols.js","cancel.js"],
     equationNumbers: { autoNumber: "AMS" },
     noUndefined: { attributes: {
                      mathcolor: "red",
                      mathbackground: "#FFEEEE",
                      mathsize: "90%" }
                  }
   },
   messageStyle: "none"
});
</script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
  </head>
  <body>
    <div id="page">
      <div id="header">
        <a id="listing" href="/posts/">all posts</a>
        <a href="/">Derivative Works</a>
      </div>
      <div id="prev-next-links">
      <a class="prev-link" href="/posts/linear-algebra-3.html">older &raquo;</a>
  
      <a class="next-link" href="/posts/linear-algebra-interlude.html">&laquo; newer</a>
  
</div>

<div style="display: none;">
\(
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\vxy}[2]{\begin{bmatrix}#1 \\ #2\end{bmatrix}}
\newcommand{\vxyz}[3]{\begin{bmatrix}#1 \\ #2 \\ #3\end{bmatrix}}
\newcommand{\rxy}[2]{\begin{bmatrix}#1 & #2\end{bmatrix}}
\newcommand{\rxyz}[3]{\begin{bmatrix}#1 & #2 & #3\end{bmatrix}}
\newcommand{\uu}[0]{\mathbf{u}}
\newcommand{\uuu}[0]{\mathbf{\hat{u}}}
\newcommand{\vv}[0]{\mathbf{v}}
\newcommand{\vvv}[0]{\mathbf{\hat{v}}}
\newcommand{\ww}[0]{\mathbf{w}}
\newcommand{\sm}[1]{\bigl[\begin{smallmatrix}#1\end{smallmatrix}\bigr]}
\newcommand{\dotp}[2]{#1~{\cdot}~#2}
\)
</div>
<div id="post">
<h1>Linear Algebra, Part 4: Elimination</h1>
<span class="post-created">posted May 2, 2013</span>
<blockquote>
<p>In <a href="/posts/linear-algebra-3.html">Part 3 of this series</a> we looked at how systems of equations can be expressed in matrix notation. In this post we'll look at how to eliminate unknowns in a system of linear equations in a matrix setting.</p>
</blockquote>
<p>For a system of linear equations we usually want to know whether the system has a solution. In particular, we want to know whether the system has a <em>unique</em> solution. A system of linear equations may have</p>
<ul>
<li><strong>No solutions</strong>: the linear structures of the system (lines, planes) do not intersect at a common point;</li>
<li><strong>A single solution</strong>: the linear structures of the system intersect at only one point;</li>
<li><strong>An infinite number of solutions</strong>: all of the linear structures intersect along a line, plane, etc., yielding an infinite number of points of intersection.</li>
</ul>
<p>The case we are interested in is the single-solution case: if the system has a single solution, we want to compute it. But systems of linear equations are difficult to solve if we try to solve for all of the unknowns simultaneously. So we need a way to simplify the process. If we can solve the system by computing one unknown at a time, then finding the solution (if it exists) should be straightforward.</p>
<h2 id="elimination-example">Elimination Example</h2>
<p>Consider the following system of equations and corresponding matrix representation:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
4x + 5y + 6z &amp;= 3 \\
7x + 8y + 10z &amp;= 11 
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 10}\vxyz{x}{y}{z} = \vxyz{2}{3}{11}
\]</span></p>
<p>The above system would be easy to solve if we could eliminate the <span class="math">\(x\)</span> term from the second equation and <span class="math">\(x\)</span> and <span class="math">\(y\)</span> terms from the third equation. We can get there by subtracting the right multiples of some of the equations from others to <em>eliminate</em> unknowns. To eliminate the <span class="math">\(x\)</span> term from the second equation, we can subtract <span class="math">\(4\)</span> times the first equation from the second (on both sides). Then for the second equation we have</p>
<p><span class="math">\[
\begin{align*}
4x + 5y + 6z - 4(x + 2y + 3z) &amp;= 3 - 4(2) \\
4x - 4x + 5y - 8y + 6z - 12z &amp;= -5 \\
-3y - 6z &amp;= -5.
\end{align*}
\]</span></p>
<p>After eliminating the <span class="math">\(x\)</span> term from the second equation we have a new system:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x - 3y - 6z &amp;= -5 \\
7x + 8y + 10z &amp;= 11
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 7 &amp; 8 &amp; 10}\vxyz{x}{y}{z} = \vxyz{2}{-5}{11}
\]</span></p>
<p>Then we can repeat the process for the <span class="math">\(x\)</span> term of the third equation by subtracting <span class="math">\(7\)</span> times the first equation from the third:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x - 3y - 6z &amp;= -5 \\
0x - 6y - 11z &amp;= -3
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 0 &amp; -6 &amp; -11}\vxyz{x}{y}{z} = \vxyz{2}{-5}{-3}
\]</span></p>
<p>Lastly, to eliminate the <span class="math">\(y\)</span> term from the third equation, we can subtract <span class="math">\(2\)</span> times the second equation from the third:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x - 3y - 6z &amp;= -5 \\
0x + 0y + z &amp;= 7
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 0 &amp; 0 &amp; 1}\vxyz{x}{y}{z} = \vxyz{2}{-5}{7}
\]</span></p>
<p>What we end up with is a simplified form of the original system in which it is easy to tell that <span class="math">\(z = 7\)</span>. Given that, we can <em>back-substitute</em> <span class="math">\(z\)</span> into the second equation to solve for <span class="math">\(y\)</span> and similarly for the first equation to solve for <span class="math">\(x\)</span>.</p>
<p>The final form of the matrix after elimination is called an <em>upper-triangular</em> matrix: it has zeros below the main diagonal. Once we have an upper-triangular matrix in a equation like the one above, it is easier to solve for all of the unknowns by back-substitution. But it might not be possible, as we'll see next.</p>
<h2 id="when-elimination-fails">When Elimination Fails</h2>
<p>The example above worked out nicely: we kept subtracting multiples of rows from other rows until we ended up with an upper-triangular matrix. But elimination doesn't always go so smoothly: what if we inadvertently eliminate <em>too many</em> unknowns during an elimination step?</p>
<p>Consider the above example, slightly modified so that the final row of the original system has <span class="math">\(9\)</span> instead of <span class="math">\(10\)</span> for the coefficient of <span class="math">\(z\)</span>:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
4x + 5y + 6z &amp;= 3 \\
7x + 8y + 9z &amp;= 11 
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9}\vxyz{x}{y}{z} = \vxyz{2}{3}{11}
\]</span></p>
<p>After elimination on the first two rows we'll have:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x - 3y - 6z &amp;= -5 \\
0x - 6y - 12z &amp;= -3
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 0 &amp; -6 &amp; -12}\vxyz{x}{y}{z} = \vxyz{2}{-5}{-3}
\]</span></p>
<p>But when we go to eliminate the <span class="math">\(y\)</span> term from the third row by subtracting <span class="math">\(2\)</span> times the second row as before, we'll get:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x - 3y - 6z &amp;= -5 \\
0x + 0y + 0z &amp;= 7
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 0 &amp; 0 &amp; 0}\vxyz{x}{y}{z} = \vxyz{2}{-5}{7}
\]</span></p>
<p>That step left us with an equation <span class="math">\(0z = 7\)</span> or <span class="math">\(0 = 7\)</span> which is clearly false. So this matrix <em>does not have any solutions</em> since there is no value of <span class="math">\(z\)</span> such that <span class="math">\(0z = 7\)</span>.</p>
<p>This kind of failure in elimination reveals an important point: all of the coefficients on the main diagonal (<span class="math">\(1\)</span>, <span class="math">\(-3\)</span>, and <span class="math">\(0\)</span> in this case) <em>must be non-zero or elimination will fail</em>. As we saw, a zero coefficient as in <span class="math">\(0z = 7\)</span> means we can't find a solution. And a zero coefficient in the case of <span class="math">\(0z = 0\)</span> means there are an <em>infinite</em> number of solutions! The coefficients of the unknowns in the matrix after elimination are called <strong>pivots</strong>. So elimination must yield <strong>non-zero pivots</strong> or we won't be able to find a unique solution.</p>
<h2 id="row-exchanges">Row Exchanges</h2>
<p>As we proceed through elimination of unknowns we construct an upper-triangular matrix. But depending on the matrix we may end up eliminating some unknowns too &quot;early&quot; in the process, yielding a matrix which can't be used for back-substituion. Consider the following example:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x + 0y - 2z &amp;= 6 \\
0x + y + z &amp;= 7
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; 0 &amp; -2 \\ 0 &amp; 1 &amp; 1}\vxyz{x}{y}{z} = \vxyz{2}{6}{7}
\]</span></p>
<p>This matrix doesn't fit the pattern we need to do elimination on the third equation; we can't use the second equation to eliminate the second unknown from the third equation. We could be given such a matrix directly or we could obtain it as the intermediate state of some process of elimination. Either way, elimination can't proceed on this matrix until we perform a <strong>row exchange</strong> and swap the second and third rows:</p>
<p><span class="math">\[
\begin{align*}
x + 2y + 3z &amp;= 2 \\
0x + y + z &amp;= 7 \\
0x + 0y - 2z &amp;= 6
\end{align*}
\hskip{0.5in}
\mat{1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; -2}\vxyz{x}{y}{z} = \vxyz{2}{7}{6}
\]</span></p>
<p>Now elimination can proceed as usual (although in this case elimination is done and we can just solve).</p>
<h2 id="why-add-or-subtract-whole-equations">Why Add or Subtract Whole Equations?</h2>
<p>At first it might seem arbitrary that we decide to start adding and subtracting equations to eliminate unknowns. But this is a perfectly legal thing to do with systems of linear equations for these two reasons:</p>
<ul>
<li>Since we are dealing with a <em>system</em> with a set of <em>common unknowns</em>, the equations are expressed in terms of the same unknown quantities. Since we are seeking a simultaneous solution to the entire system, <span class="math">\(x\)</span> in one equation is the same <span class="math">\(x\)</span> in the other equations.</li>
<li>Since we are dealing with <em>equations</em> (i.e. forms such as <span class="math">\(a = b\)</span>), we can <em>assume</em> that the quantities <span class="math">\(a\)</span> and <span class="math">\(b\)</span> are equal as stated, so it's perfectly legal to add or subtract them from other equations <em>without changing the truth of the equations</em>. And as long as we've multiplied both sides before adding or subtracting, we've preserved the consistency of the equations we're working with.</li>
</ul>
<h2 id="afterword">Afterword</h2>
<p>In this installment we've looked at the process of elimination for solving systems of linear equations. We've also looked at elimination's two failure modes: zero pivots (permanent failure) and row ordering problems (temporary failure). Next time we'll look at matrix multiplication and how we can express elimination as a product of matrices.</p>
</div>
<div id="disqus_thread"></div>
<script type="text/javascript">
  /**
    * var disqus_identifier; [Optional but recommended: Define a unique identifier (e.g. post id or slug) for this thread] 
    */
  var disqus_identifier = "linear-algebra-4";
  (function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = 'http://derivativeworks.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>
  Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript=derivativeworks">comments.</a>
</noscript>
      <div id="footer">
        Copyright &copy; 2010-2013 Jonathan Daugherty
      </div>
    </div>
  </body>
</html>