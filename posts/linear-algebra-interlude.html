<html>
  <head>
    <title>Linear Algebra: Interlude - Derivative Works</title>
    <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/3.2.0/build/cssreset/reset-min.css">
    <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/3.2.0/build/cssbase/base-min.css">
    <link rel="stylesheet" type="text/css" href="http://yui.yahooapis.com/3.2.0/build/cssfonts/fonts-min.css">
    <link rel="stylesheet" type="text/css" href="/stylesheets/stylesheet.css"/>
    <link rel="alternate" type="application/rss+xml" href="http://jtdaugherty.github.io/feed.xml"/>
    <script type="text/x-mathjax-config">
MathJax.Hub.Config(
  {"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
   tex2jax: {
     element: null,
     preview: "none",
     skipTags: ["script","noscript","style","textarea","pre","code"],
     inlineMath: [
       ['$', '$'],
       ["\\(", "\\)"],
     ],
     displayMath: [
       ['$$', '$$'],
       ["\\[", "\\]"],
     ],
     processEscapes: true,
     ignoreClass: "tex2jax_ignore|dno"
   },
   TeX: {
     extensions: ["AMSmath.js","AMSsymbols.js","cancel.js"],
     equationNumbers: { autoNumber: "AMS" },
     noUndefined: { attributes: {
                      mathcolor: "red",
                      mathbackground: "#FFEEEE",
                      mathsize: "90%" }
                  }
   },
   messageStyle: "none"
});
</script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
  </head>
  <body>
    <div id="page">
      <div id="header">
        <a id="listing" href="/posts/">all posts</a>
        <a href="/">Derivative Works</a>
      </div>
      <div id="prev-next-links">
      <a class="prev-link" href="/posts/linear-algebra-4.html">older &raquo;</a>
  
      <a class="next-link-subdued">&laquo; newer</a>
  
</div>

<div style="display: none;">
\(
\newcommand{\xx}[0]{\mathbf{x}}
\newcommand{\bb}[0]{\mathbf{b}}
\newcommand{\AA}[0]{\mathbf{A}}
\newcommand{\BB}[0]{\mathbf{B}}
\newcommand{\CC}[0]{\mathbf{C}}
\newcommand{\II}[0]{\mathbf{I}}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\vxyz}[3]{\begin{bmatrix}#1 \\ #2 \\ #3\end{bmatrix}}
\newcommand{\rxyz}[3]{\begin{bmatrix}#1 & #2 & #3\end{bmatrix}}
\newcommand{\sm}[1]{\bigl[\begin{smallmatrix}#1\end{smallmatrix}\bigr]}
\)
</div>
<div id="post">
<h1>Linear Algebra: Interlude</h1>
<span class="post-created">posted August 1, 2013</span>
<blockquote>
<p>In this post we look at the idea that matrices can be viewed as functions. We bother doing this because it puts some of the later ideas in context and allows us to re-use existing ideas (injectivity, surjectivity, and bijectivity) to think about matrices.</p>
</blockquote>
<h2 id="matrix-function">Matrix <span class="math">\(=\)</span> Function</h2>
<p>In this post I'd like to be explicit about an idea which may or may not be covered by your linear algebra material. I find that keeping this idea in mind can really help motivate the ways in which we talk about matrices and why we care about looking for inverses. The idea is summed up as follows:</p>
<blockquote>
<p>An <span class="math">\(m \times n\)</span> matrix <span class="math">\(\AA\)</span> times a vector <span class="math">\(\xx\)</span> can be thought of as a function of that vector which produces a new vector, i.e., <span class="math">\(\AA : \mathbb{R}^n \to \mathbb{R}^m\)</span>.</p>
</blockquote>
<p>If matrices are functions, then matrix multiplication (i.e., successive application of a matrix) is analogous to function composition:</p>
<p><span class="math">\[
\begin{align*}
\AA\BB\CC &amp;= \AA \circ \BB \circ \CC \\
\AA\BB\CC\xx &amp;= \AA(\BB(\CC(\xx))).
\end{align*}
\]</span></p>
<p>And just as function composition is associative, so is matrix multiplication:</p>
<p><span class="math">\[
\begin{align*}
f \circ (g \circ h) &amp;= (f \circ g) \circ h; \\
\AA(\BB\CC) &amp;= (\AA\BB)\CC.
\end{align*}
\]</span></p>
<p>And lastly, the square identity matrix <span class="math">\(\II\)</span> is just the identity function for the appropriate vector domain.</p>
<h2 id="invertibility">Invertibility</h2>
<p>We spend a lot of time trying to determine whether matrices are invertible. We learn about quick and easy ways to spot flaws that tell us whether this is impossible for a given matrix, or a process to find the inverse if it exists. Why do we care about inverses?</p>
<p>If we're given an equation <span class="math">\(f(x) = y\)</span> and we're asked to solve for <span class="math">\(x\)</span>, the process of finding <span class="math">\(x\)</span> essentially leads us to find the inverse of <span class="math">\(f\)</span>, written <span class="math">\(f^{-1}\)</span>, such that if <span class="math">\(f(x) = y\)</span>, then <span class="math">\(x = f^{-1}(y)\)</span>. If we have such a function <span class="math">\(f^{-1}\)</span>, then in the future if we're asked to find some <span class="math">\(x\)</span> given a <span class="math">\(y\)</span>, we can just apply the inverse to compute <span class="math">\(x\)</span> directly rather than algebraically manipulating <span class="math">\(f(x) = y\)</span> until we can solve for <span class="math">\(x\)</span>. The same principle applies for matrices.</p>
<p>In previous posts we looked at the process for solving a system of equations in the form <span class="math">\(\AA\xx = \bb\)</span>. The process is a bit laborious and can be computationally expensive if <span class="math">\(\AA\)</span> has many rows and columns. But if we think of matrices as functions, then we should end up with</p>
<p><span class="math">\[
\begin{align*}
\AA(\xx) &amp;= \bb; \\
\AA^{-1}(\bb) &amp;= \xx.
\end{align*}
\]</span></p>
<p>That would make solving such systems straightforward, and we could reuse <span class="math">\(\AA^{-1}\)</span> instead of repeating the elimination process for every <span class="math">\(\AA\xx = \bb\)</span>. But not all functions are invertible, and the same is true for matrices; a function is invertible if and only if it is both <em>surjective</em> and <em>injective</em>. Now we'll look at these properties in the context of matrices.</p>
<h2 id="surjective-matrices">Surjective Matrices</h2>
<p>A function is <em>surjective</em> if the function maps inputs to all elements of its <em>codomain</em> without leaving any out. An example of a surjective function is <span class="math">\(f : \mathbb{R} \to \mathbb{R} = x + 1\)</span>: the entirety of the codomain <span class="math">\(\mathbb{R}\)</span> is covered by <span class="math">\(f\)</span>, i.e., there are no values of <span class="math">\(y\)</span> in <span class="math">\(\mathbb{R}\)</span> that don't have some corresponding <span class="math">\(x\)</span> in <span class="math">\(\mathbb{R}\)</span>. An example of a function which is <em>not</em> surjective is <span class="math">\(f : \mathbb{R} \to \mathbb{R} = |x|\)</span>. There are an infinite number of values in <span class="math">\(\mathbb{R}\)</span> that have no corresponding <span class="math">\(x\)</span>: all <span class="math">\(y &lt; 0\)</span>.</p>
<p>A surjective <span class="math">\(m \times n\)</span> matrix is one which takes all its input <span class="math">\(n\)</span>-vectors into the entirety of <span class="math">\(\mathbb{R}^m\)</span>. An example of such a matrix is the <span class="math">\(n \times n\)</span> identity matrix <span class="math">\(\II_n\)</span>: any vector in <span class="math">\(\mathbb{R}^n\)</span> can be expressed as a sum of the column vectors in <span class="math">\(\II_n\)</span>, so we can find some input vector <span class="math">\(\xx\)</span> to which it corresponds. (Easily: <span class="math">\(\II\xx = \xx\)</span> for any <span class="math">\(\xx\)</span>.)</p>
<p>An example of a matrix which is <em>not</em> surjective is a matrix whose columns combine to form a subset of <span class="math">\(\mathbb{R}^m\)</span>, such as a line or a plane, e.g.,</p>
<p><span class="math">\[
\AA = \mat{1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1}.
\]</span></p>
<p>No matter the input <span class="math">\(x\)</span>, the above matrix <span class="math">\(\AA\)</span> will yield only vectors in the <span class="math">\(x\)</span>-<span class="math">\(z\)</span> plane. Vectors such as <span class="math">\(\bb = \rxyz{1}{2}{1}\)</span> have no corresponding <span class="math">\(\xx\)</span>.</p>
<h2 id="injective-matrices">Injective Matrices</h2>
<p>A function is <em>injective</em> if it maps each input <span class="math">\(x\)</span> to a <em>unique</em> value <span class="math">\(y\)</span>, i.e., <span class="math">\(f(x_1) = f(x_2) \implies x_1 = x_2\)</span>.</p>
<p>An injective <span class="math">\(m \times n\)</span> matrix <span class="math">\(\AA\)</span> behaves the same way: for any result vector <span class="math">\(\bb\)</span>, there is <em>at most</em> a single <span class="math">\(\xx\)</span> such that <span class="math">\(\AA\xx = \bb\)</span>.</p>
<p>An example of an injective matrix is <span class="math">\(\II_n\)</span>: for any right hand side <span class="math">\(\bb\)</span> there is exactly one <span class="math">\(\xx\)</span> that maps to it. An example of a matrix which is <em>not</em> injective is, again, <span class="math">\(\AA\)</span> given above in the surjective case: an infinitude of vectors <span class="math">\(\xx = \rxyz{a}{-a}{1}\)</span> for any <span class="math">\(a\)</span> map to the same vector <span class="math">\(\xx = \rxyz{0}{0}{1}\)</span>.</p>
<h2 id="bijective-invertible-matrices">Bijective (Invertible) Matrices</h2>
<p>If a function is both injective and surjective, it can be inverted. So it is with matrices: all of the codomain <span class="math">\(\mathbb{R}^m\)</span> is covered and every vector in <span class="math">\(\mathbb{R}^m\)</span> has a unique corresponding vector in <span class="math">\(\mathbb{R}^n\)</span>.</p>
<p>A lot of material in linear algebra texts deals with the various ways in which matrices fail to be invertible. But once we have the inverse <span class="math">\(\AA^{-1}\)</span> of a matrix <span class="math">\(\AA\)</span>, it's trivial to solve a system of equations involving <span class="math">\(\AA\)</span>. Given <span class="math">\(\AA\xx = \bb\)</span>, we can just multiply <span class="math">\(\AA^{-1}\bb\)</span> to obtain <span class="math">\(\xx\)</span>.</p>
<h2 id="afterword">Afterword</h2>
<p>Now that we have drawn some parallels between matrices and functions, we should be able to better understand upcoming material. Next time I'll resume the series with coverage of elimination as matrix multiplication.</p>
</div>
<div id="disqus_thread"></div>
<script type="text/javascript">
  /**
    * var disqus_identifier; [Optional but recommended: Define a unique identifier (e.g. post id or slug) for this thread] 
    */
  var disqus_identifier = "linear-algebra-interlude";
  (function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = 'http://derivativeworks.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>
  Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript=derivativeworks">comments.</a>
</noscript>
      <div id="footer">
        Copyright &copy; 2010-2013 Jonathan Daugherty
      </div>
    </div>
  </body>
</html>